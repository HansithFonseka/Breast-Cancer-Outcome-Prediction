
# ðŸŽ—ï¸ Breast Cancer Outcome Prediction

Welcome to the **Breast Cancer Outcome Prediction** project! ðŸš€  
This machine learning project predicts breast cancer patient outcomes using classification and regression models.  
The goal is to predict **mortality status (Alive/Dead)** and **survival months**, achieving **85.3% classification accuracy** and an **80-month survival prediction**. ðŸ©º

---

## ðŸ“– Project Overview

This project tackles two case studies using a breast cancer dataset:

### Case Study A: Classification ðŸ“Š
Predicts whether a patient is Alive or Dead based on features like **Age**, **T_Stage**, and **Tumor_Size**.  
**Models used:** Logistic Regression, Naive Bayes, KNN, and a Voting Classifier ensemble.

### Case Study B: Regression ðŸ“ˆ
Predicts **Survival Months** using Decision Tree Regressors, with a focus on **interpretability and generalization**.

ðŸ§¾ The project includes **three Jupyter notebooks** for data preprocessing, modeling, and evaluation, along with a **detailed report** summarizing the findings.

---

## ðŸ› ï¸ Technologies Used

- Python ðŸ  
- scikit-learn *(Logistic Regression, KNN, Naive Bayes, Decision Trees, Voting Classifier)*  
- pandas & NumPy *(data manipulation)*  
- Matplotlib & Seaborn *(visualization)* ðŸ“‰  
- SMOTE *(handling class imbalance)*  
- GridSearchCV *(hyperparameter tuning)*  
- Jupyter Notebooks *(development environment)* ðŸ““

---

## ðŸŒŸ Key Achievements

- **High Classification Accuracy:** Achieved **85.3% accuracy** with Logistic Regression for predicting mortality status, with **0.63 recall for the minority class (Dead)** using SMOTE. âœ…  
- **Survival Prediction:** Predicted **80 months survival** for a new patient using a Decision Tree Regressor (*max_depth=4*), with a **Mean Absolute Error (MAE)** of **19.05 months**. â³  
- **Robust Preprocessing:** Handled missing values, outliers (using IQR), and categorical encoding. ðŸ§¹  

---

## ðŸ“‚ Repository Structure

```
Breast-Cancer-Outcome-Prediction/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Final_Python_Notebook_1.ipynb    # Data preprocessing ðŸ§¼
â”‚   â”œâ”€â”€ Final_Python_Notebook_2.ipynb    # Classification with SMOTE ðŸ“Š
â”‚   â”œâ”€â”€ Final_Python_Notebook_3.ipynb    # Ensemble & regression ðŸ“ˆ
â”œâ”€â”€ requirements.txt                     # Python dependencies ðŸ“‹
â”œâ”€â”€ README.md                            # You're here! ðŸ‘‹
```

> âš ï¸ **Note**: The dataset is not included due to academic/privacy restrictions.  
> You can use similar datasets (e.g., from [SEER](https://seer.cancer.gov/survivaltime)) to replicate the results.

---

## ðŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/Breast-Cancer-Outcome-Prediction.git
cd Breast-Cancer-Outcome-Prediction
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Prepare the Data

- Obtain a breast cancer dataset with features like:  
  `Age`, `Sex`, `T_Stage`, `Tumor_Size`, `Mortality_Status`, `Survival_Months`
- Place the dataset in a **`data/`** folder and update file paths in the notebooks.

### 4ï¸âƒ£ Run the Notebooks

```bash
jupyter notebook
```

> Start with `Final_Python_Notebook_1.ipynb` for preprocessing, then proceed to `Notebook_2` and `Notebook_3`.

---

## ðŸ“Š Results & Insights

### ðŸ”¹ Case Study A: Classification

- **Best Model:** Logistic Regression (Notebook 2)  
  - Accuracy: **85.3%**  
  - Precision: **0.59**  
  - Recall (Dead): **0.63** (using SMOTE)
- **Ensemble:** Voting Classifier (Notebook 3)  
  - Accuracy: **84%**  
  - Improved **AUC**
- **Challenge:** Class imbalance (~85% Alive, ~15% Dead)  
  - Addressed using **SMOTE**

### ðŸ”¹ Case Study B: Regression

- **Best Model:** Decision Tree Regressor (*max_depth=4*, Notebook 3)  
  - MAE: **19.05**  
  - RÂ² Score: **-0.10**
- **Prediction:** **80 months** survival for patient **B002565**  
- **Insight:** Treeâ€™s limited depth prevented overfitting but caused underfitting (RÂ² < 0).

---

## ðŸ”§ Future Improvements

- ðŸ§ª **Advanced Models:** Try Random Forest or XGBoost  
- âš–ï¸ **Class Imbalance:** Apply SMOTE consistently  
- ðŸ“‰ **Regression Fit:** Tune Decision Tree depth or try ensemble regressors  
- ðŸ§‘â€ðŸ”¬ **Feature Engineering:** Drop noisy features like Occupation  

---

## ðŸ“š References

- Kuhn, M., & Johnson, K. (2013). *Applied Predictive Modeling*. Springer.  
- SEER Cancer Data: [https://seer.cancer.gov/survivaltime](https://seer.cancer.gov/survivaltime)

---

## ðŸ“„ License

This project is licensed under the **MIT License** â€“ feel free to use and modify it! ðŸŒŸ

---

## ðŸ™‹ Connect with Me

ðŸ“§ **Email:** hansithlochana@gmail.com  
ðŸ”— **LinkedIn:** [linkedin.com/in/hansith-fonseka](https://www.linkedin.com/in/hansith-fonseka)  
ðŸ™ **GitHub:** [github.com/HansithFonseka](https://github.com/HansithFonseka)
